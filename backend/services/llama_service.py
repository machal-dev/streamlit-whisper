from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# âœ… Meta ìŠ¹ì¸ì´ í•„ìš” ì—†ëŠ” ì˜¤í”ˆëª¨ë¸
model_id = "NousResearch/Llama-2-7b-chat-hf"

# âœ… í† í° ì¸ì¦ ì—†ì´ë„ ë™ì‘ (í•„ìš”ì‹œ huggingface-cli login ë¨¼ì € í•´ì¤˜ì•¼ í•¨)
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# âœ… í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬ì„±
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

# ğŸ” í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
prompt = "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\në‹µ:"
outputs = generator(prompt, max_new_tokens=64, do_sample=True, temperature=0.7)

# ğŸ”¥ ì¶œë ¥
print("ğŸ”¥ ê²°ê³¼:", outputs[0]["generated_text"])
