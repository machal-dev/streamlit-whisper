from fastapi import APIRouter
from pydantic import BaseModel, Field
from backend.llama_pack.service import generate_text
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

class PromptRequest(BaseModel):
    prompt: str = Field(..., description="ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì˜ˆ: 'ì‚¼êµ­ì§€ë¥¼ ìš”ì•½í•´ì¤˜')")
    max_new_tokens: int = Field(64, description="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜")
    temperature: float = Field(0.7, description="ì°½ì˜ì„± ì •ë„ (0~1)")
    do_sample: bool = Field(True, description="ìƒ˜í”Œë§ ì—¬ë¶€")
    extract_after_answer: bool = Field(True, description="'ë‹µ:' ì´í›„ë§Œ ì¶”ì¶œí• ì§€ ì—¬ë¶€")

@router.post("/generate")
def llama_generate(req: PromptRequest):
    logger.info("ğŸŸ¦ [API] POST /llama/generate ìš”ì²­ ìˆ˜ì‹ ")
    logger.debug(f"[í”„ë¡¬í”„íŠ¸] {req.prompt[:100]}...")

    result = generate_text(
        prompt=req.prompt,
        max_new_tokens=req.max_new_tokens,
        temperature=req.temperature,
        do_sample=req.do_sample,
        extract_after_answer=req.extract_after_answer
    )

    logger.info("ğŸŸ¢ [API] LLaMA ì‘ë‹µ ìƒì„± ì™„ë£Œ")
    return {"result": result}
