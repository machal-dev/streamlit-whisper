import streamlit as st
import requests
import tempfile
import os

def render():
    from backend.llama_pack.service import generate_text

    st.header("ğŸ™ï¸ ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸ â†’ LLaMA ì‘ë‹µ")

    # ì˜¤ë””ì˜¤ ì—…ë¡œë“œ UI
    uploaded_file = st.file_uploader(
        "ìŒì„± íŒŒì¼ ì—…ë¡œë“œ (mp3, wav ë“±)",
        type=["mp3", "wav", "m4a"],
        key="voice_to_llm_file_uploader"
    )

    language = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        options=["ko", "en"],
        index=0,
        key="voice_to_llm_language_select"
    )

    max_tokens = st.slider(
        "ì‘ë‹µ ìµœëŒ€ í† í° ìˆ˜",
        16, 512, 64,
        key="voice_to_llm_max_tokens"
    )

    temperature = st.slider(
        "ì°½ì˜ì„± (temperature)",
        0.0, 1.0, 0.7,
        key="voice_to_llm_temperature"
    )

    do_sample = st.checkbox(
        "ìƒ˜í”Œë§ ì‚¬ìš©",
        value=True,
        key="voice_to_llm_do_sample"
    )

    extract_answer = st.checkbox(
        "ë‹µ: ì´í›„ë§Œ ì¶”ì¶œ",
        value=True,
        key="voice_to_llm_extract_answer"
    )

    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            tmp.write(uploaded_file.read())
            temp_path = tmp.name

        st.audio(temp_path, format="audio/mp3", start_time=0)

        if st.button("ğŸ§  ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°", key="voice_to_llm_submit_button"):
            try:
                with open(temp_path, "rb") as f:
                    files = {"file": (os.path.basename(temp_path), f, uploaded_file.type)}
                    whisper_res = requests.post(
                        "http://localhost:8000/whisper/transcribe",
                        files=files,
                        data={"language": language},
                    )

                if whisper_res.status_code == 200:
                    prompt = whisper_res.json()["result"]
                    st.info(f"ğŸ“ ì¸ì‹ëœ ì§ˆë¬¸: {prompt}", icon="ğŸ“")

                    result = generate_text(
                        prompt=prompt,
                        max_new_tokens=max_tokens,
                        temperature=temperature,
                        do_sample=do_sample,
                        extract_after_answer=extract_answer
                    )
                    st.success("ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    st.text_area("ğŸ“„ ì‘ë‹µ ê²°ê³¼", result, height=200, key="llama_only_result_output")

                    # GPU ë¶€í•˜ë¡œ ì¸í•œ í…ŒìŠ¤íŠ¸ ë¶ˆê°€ ìƒí™© 
                    # llama_res = requests.post(
                    #     "http://localhost:8000/llama/generate",
                    #     json={
                    #         "prompt": prompt,
                    #         "max_new_tokens": max_tokens,
                    #         "temperature": temperature,
                    #         "do_sample": do_sample,
                    #         "extract_after_answer": extract_answer,
                    #     },
                    # )
                    #
                    # if llama_res.status_code == 200:
                    #     answer = llama_res.json()["result"]
                    #     st.success("âœ… ì‘ë‹µ ì™„ë£Œ")
                    #     st.text_area("ğŸ“„ LLaMA ì‘ë‹µ", answer, height=200, key="voice_to_llm_output")
                    # else:
                    #     st.error(f"LLaMA ì˜¤ë¥˜: {llama_res.text}")
                else:
                    st.error(f"Whisper ì˜¤ë¥˜: {whisper_res.text}")
            except Exception as e:
                st.error(f"ì˜ˆì™¸ ë°œìƒ: {e}")
            finally:
                os.remove(temp_path)
