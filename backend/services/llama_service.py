from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

print("ğŸŸ¡ [LLaMA ì„œë¹„ìŠ¤] ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

model_id = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
)

print("âœ… [LLaMA ì„œë¹„ìŠ¤] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

print("âœ… [LLaMA ì„œë¹„ìŠ¤] íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")

def generate_text(prompt: str, max_new_tokens=64, temperature=0.7, do_sample=True) -> str:
    print(f"ğŸ§  [LLaMA] í”„ë¡¬í”„íŠ¸ ìˆ˜ì‹ : {prompt}")
    outputs = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=do_sample
    )

    result = outputs[0]["generated_text"]
    print(f"ğŸŸ¢ [LLaMA ì„œë¹„ìŠ¤] ê²°ê³¼ ìƒì„± ì™„ë£Œ")
    return result
