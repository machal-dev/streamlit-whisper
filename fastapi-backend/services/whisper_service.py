import whisper
import torch
import asyncio
import os

device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("medium", device=device)


# ðŸŽ¯ ë™ê¸° í•¨ìˆ˜ (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë“±ì—ì„œ ë°”ë¡œ í˜¸ì¶œ ê°€ëŠ¥)
def transcribe_sync(path: str, translate: bool = False) -> str:
    task = "translate" if translate else "transcribe"
    result = model.transcribe(path, language="ko", task=task)
    return result["text"]


# âš¡ ë¹„ë™ê¸° í•¨ìˆ˜ (FastAPI ë“±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
async def transcribe_async(path: str, translate: bool = False) -> str:
    loop = asyncio.get_event_loop()
    task = "translate" if translate else "transcribe"
    result = await loop.run_in_executor(None, lambda: model.transcribe(path, language="ko", task=task))
    return result["text"]
