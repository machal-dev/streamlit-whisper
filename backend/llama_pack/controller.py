# âœ… llama_pack/controller.py
"""
LLaMA ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬
- ëª¨ë¸ ë¡œë”©, ì–¸ë¡œë”©, ìƒíƒœ í™•ì¸ (Lazy Loading)
"""

import logging
import torch
from backend.llama_pack import config
from transformers import AutoTokenizer, pipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

logger = logging.getLogger(__name__)

# ì „ì—­ ìƒíƒœ ì €ì¥ìš©
_model = None
_tokenizer = None
_generator = None
_is_loaded = False

def load_model():
    global _model, _tokenizer, _generator, _is_loaded

    if _is_loaded:
        logger.info(f"{config.LOG_TAG} ğŸ”„ ì´ë¯¸ ë¡œë”©ëœ ëª¨ë¸ ì‚¬ìš©")
        return

    logger.info(f"{config.LOG_TAG} ğŸŸ¡ ëª¨ë¸ ë¡œë”© ì‹œì‘...")

    _tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID, use_fast=True)

    quantize_config = BaseQuantizeConfig(
        bits=4,
        group_size=128,
        desc_act=False
    )

    _model = AutoGPTQForCausalLM.from_pretrained(
        config.MODEL_ID,
        quantize_config=quantize_config,
        use_safetensors=True,
        trust_remote_code=True,
        # GPTQ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ë•ŒëŠ” deviceë¥¼ from_pretrained()ì— ì§ì ‘ ë„˜ê¸°ë©´ ì•ˆ ë˜ê³ , pipeline() ìª½ì—ì„œ ì²˜ë¦¬
        # device=config.DEVICE
    )

    _generator = pipeline(
        "text-generation",
        model=_model,
        tokenizer=_tokenizer,
        device=0  # ë˜ëŠ” device_map="auto"
    )

    _is_loaded = True
    logger.info(f"{config.LOG_TAG} âœ… ëª¨ë¸ ë° íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ")

def unload_model():
    global _model, _tokenizer, _generator, _is_loaded

    if not _is_loaded:
        logger.info(f"{config.LOG_TAG} ğŸš« ì–¸ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    del _model
    del _tokenizer
    del _generator
    torch.cuda.empty_cache()
    _is_loaded = False
    logger.info(f"{config.LOG_TAG} ğŸ§¹ ëª¨ë¸ ì–¸ë¡œë“œ ë° GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def is_model_loaded() -> bool:
    return _is_loaded

def get_generator():
    if not _is_loaded:
        raise RuntimeError("LLaMA ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
    return _generator
