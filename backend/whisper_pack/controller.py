# backend/whisper_pack/controller.py

"""
Whisper ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬
- ëª¨ë¸ ë¡œë”©, ì–¸ë¡œë”©, ìƒíƒœ í™•ì¸ (Lazy Loading)
"""

import logging
import torch
import whisper
from backend.whisper_pack import config

logger = logging.getLogger(__name__)

# ì „ì—­ ê°ì²´
_model = None
_is_loaded = False


def load_model():
    """Whisper ëª¨ë¸ì„ lazy-loading ë°©ì‹ìœ¼ë¡œ ë¡œë”©í•œë‹¤."""
    global _model, _is_loaded

    if _is_loaded:
        logger.info(f"{config.LOG_TAG} ğŸ”„ ì´ë¯¸ ë¡œë”©ëœ ëª¨ë¸ ì‚¬ìš©")
        return

    logger.info(f"{config.LOG_TAG} ğŸŸ¡ ëª¨ë¸ ë¡œë”© ì¤‘...")

    _model = whisper.load_model(config.MODEL_ID, device=config.DEVICE)

    _is_loaded = True
    logger.info(f"{config.LOG_TAG} âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")


def unload_model():
    """Whisper ëª¨ë¸ì„ ì–¸ë¡œë”©í•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•œë‹¤."""
    global _model, _is_loaded

    if not _is_loaded:
        logger.info(f"{config.LOG_TAG} ğŸš« ì–¸ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    del _model
    torch.cuda.empty_cache()
    _is_loaded = False
    logger.info(f"{config.LOG_TAG} ğŸ§¹ ëª¨ë¸ ì–¸ë¡œë“œ ë° GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def is_model_loaded() -> bool:
    """ëª¨ë¸ ë¡œë”© ì—¬ë¶€ í™•ì¸"""
    return _is_loaded


def get_model():
    """Whisper ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜. ë¯¸ë¡œë”© ì‹œ ì˜ˆì™¸ ë°œìƒ."""
    if not _is_loaded:
        raise RuntimeError("Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
    return _model
