import torch
import logging
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

logger = logging.getLogger(__name__)

logger.info("ğŸŸ¡ [LLaMA ì„œë¹„ìŠ¤] ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    device_map="auto",
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

logger.info("âœ… [LLaMA ì„œë¹„ìŠ¤] ëª¨ë¸ ë° íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ")


def generate_text(
    prompt: str,
    max_new_tokens: int = 64,
    temperature: float = 0.7,
    do_sample: bool = True,
    system_prompt: str = None,
    extract_after_answer: bool = True
) -> str:
    """
    LLaMA í…ìŠ¤íŠ¸ ìƒì„±
    - prompt: ì‚¬ìš©ì ì…ë ¥
    - extract_after_answer: 'ë‹µ:' ì´í›„ë§Œ ì¶”ì¶œí• ì§€ ì—¬ë¶€
    """

    logger.debug(f"[ğŸ§  LLaMA] í”„ë¡¬í”„íŠ¸ ìˆ˜ì‹ :\n{prompt}")

    final_prompt = prompt.strip()
    if system_prompt:
        final_prompt = system_prompt.strip() + "\n\n" + prompt.strip()

    outputs = generator(
        final_prompt,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=do_sample
    )

    result = outputs[0]["generated_text"]

    logger.debug("[ğŸŸ¢ LLaMA] ì›ë³¸ ì‘ë‹µ:\n%s", result)

    if extract_after_answer and "ë‹µ:" in result:
        result = result.split("ë‹µ:", 1)[-1].strip()

    return result
